{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a46973b1-c18b-4ec2-b1c0-af778e1665c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# d1\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load pre-trained model\n",
    "model_path = \"./models/helmet_vit\"\n",
    "helmet_detector = pipeline(\"image-classification\", model=model_path)\n",
    "\n",
    "# Create upload folder\n",
    "UPLOAD_FOLDER = \"./Media\"\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "app.config[\"UPLOAD_FOLDER\"] = UPLOAD_FOLDER\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"error\": \"No file uploaded\"}), 400\n",
    "\n",
    "    file = request.files[\"file\"]\n",
    "    if file.filename == \"\":\n",
    "        return jsonify({\"error\": \"No file selected\"}), 400\n",
    "\n",
    "    # Save the uploaded image\n",
    "    file_path = os.path.join(app.config[\"UPLOAD_FOLDER\"], file.filename)\n",
    "    file.save(file_path)\n",
    "\n",
    "    # Load and predict\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    result = helmet_detector(image)\n",
    "\n",
    "    # Delete the file after prediction\n",
    "    os.remove(file_path)\n",
    "\n",
    "    return jsonify({\"prediction\": result})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa69344-8822-4555-9c30-31a6592e7b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D2\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load model and processor\n",
    "model_path = \"./models/helmet_vit\"\n",
    "model = ViTForImageClassification.from_pretrained(model_path, output_hidden_states=True)\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "def detect_helmets(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_width, original_height = image.size\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract hidden states and generate bounding boxes\n",
    "    hidden_states = outputs.hidden_states[-1]  # Get the last layer's hidden states\n",
    "    logits = outputs.logits[0].softmax(dim=-1)  # Confidence scores for each class\n",
    "\n",
    "    # Generate bounding boxes dynamically\n",
    "    threshold = 0.5  # Confidence threshold for filtering predictions\n",
    "    results = []\n",
    "    for idx, logit in enumerate(logits):\n",
    "        if logit.max() >= threshold:\n",
    "            label = \"With Helmet\" if logit.argmax() == 0 else \"Without Helmet\"\n",
    "            x1, y1, x2, y2 = np.random.randint(0, original_width // 2), np.random.randint(\n",
    "                0, original_height // 2\n",
    "            ), np.random.randint(\n",
    "                original_width // 2, original_width\n",
    "            ), np.random.randint(\n",
    "                original_height // 2, original_height\n",
    "            )  # Replace with proper bounding box logic based on hidden_states\n",
    "            results.append({\"bbox\": [x1, y1, x2, y2], \"label\": label, \"score\": logit.max().item()})\n",
    "\n",
    "    return results\n",
    "\n",
    "def draw_bounding_boxes(image_path, predictions):\n",
    "    # Open the image\n",
    "    image = cv2.imread(image_path)\n",
    "    for pred in predictions:\n",
    "        x1, y1, x2, y2 = pred['bbox']\n",
    "        label = f\"{pred['label']} ({pred['score']:.2f})\"\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        # Draw text\n",
    "        font_scale = 0.5\n",
    "        font_thickness = 1\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness\n",
    "        )\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (x1, y1 - text_height - baseline),\n",
    "            (x1 + text_width, y1),\n",
    "            (0, 255, 0),\n",
    "            thickness=cv2.FILLED,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            label,\n",
    "            (x1, y1 - baseline),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            font_scale,\n",
    "            (0, 0, 0),\n",
    "            font_thickness,\n",
    "        )\n",
    "\n",
    "    return image\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_path = \"./Media/riders_1.jpg\"\n",
    "    output_image_path = \"./output/detected_riders.jpg\"\n",
    "\n",
    "    try:\n",
    "        # Detect helmets\n",
    "        predictions = detect_helmets(input_image_path)\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        output_image = draw_bounding_boxes(input_image_path, predictions)\n",
    "\n",
    "        # Save and display output\n",
    "        cv2.imwrite(output_image_path, output_image)\n",
    "        cv2.imshow(\"Helmet Detection\", output_image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad3d4d1-fb34-4de5-88a4-6a7161358de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D3\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the ViT model and image processor\n",
    "model_path = \"./models/helmet_vit\"\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_faces(image):\n",
    "    \"\"\"Detect faces in an image using Haar Cascade.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    return faces\n",
    "\n",
    "def classify_helmet(image, face_coordinates):\n",
    "    \"\"\"Classify whether the face detected has a helmet.\"\"\"\n",
    "    predictions = []\n",
    "    for (x, y, w, h) in face_coordinates:\n",
    "        face_img = image[y:y+h, x:x+w]  # Crop the face region\n",
    "        face_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)).resize((224, 224))\n",
    "        inputs = image_processor(images=face_pil, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits[0].softmax(dim=-1)\n",
    "        label = \"With Helmet\" if logits.argmax() == 0 else \"Without Helmet\"\n",
    "        score = logits.max().item()\n",
    "        predictions.append({\"bbox\": (x, y, w, h), \"label\": label, \"score\": score})\n",
    "    return predictions\n",
    "\n",
    "def draw_results(image, results):\n",
    "    \"\"\"Draw bounding boxes and labels on the image.\"\"\"\n",
    "    for result in results:\n",
    "        x, y, w, h = result['bbox']\n",
    "        label = f\"{result['label']} ({result['score']:.2f})\"\n",
    "        color = (0, 255, 0) if \"With Helmet\" in result['label'] else (0, 0, 255)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n",
    "\n",
    "        # Draw label\n",
    "        cv2.putText(image, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return image\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_path = \"./Media/riders_1.jpg\"\n",
    "    output_image_path = \"./output/detected_faces_helmets.jpg\"\n",
    "\n",
    "    # Read the input image\n",
    "    img = cv2.imread(input_image_path)\n",
    "\n",
    "    # Detect faces\n",
    "    face_boxes = detect_faces(img)\n",
    "\n",
    "    # Classify each detected face\n",
    "    results = classify_helmet(img, face_boxes)\n",
    "\n",
    "    # Draw results\n",
    "    output_image = draw_results(img, results)\n",
    "\n",
    "    # Save and display the output\n",
    "    cv2.imwrite(output_image_path, output_image)\n",
    "    cv2.imshow(\"Helmet Detection\", output_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71159a35-c23a-44b3-82c9-794c5935710e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:189: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#D4\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# Load Vision Transformer model and image processor\n",
    "model_path = \"./models/helmet_vit\"\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Initialize MTCNN detector\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "def detect_faces_with_mtcnn(image):\n",
    "    \"\"\"Detect faces using MTCNN.\"\"\"\n",
    "    results = mtcnn.detect_faces(image)\n",
    "    faces = []\n",
    "    for res in results:\n",
    "        x, y, w, h = res['box']\n",
    "        faces.append((x, y, w, h))\n",
    "    return faces\n",
    "\n",
    "def classify_helmet(image, face_coordinates):\n",
    "    \"\"\"Classify whether the detected face has a helmet.\"\"\"\n",
    "    predictions = []\n",
    "    for (x, y, w, h) in face_coordinates:\n",
    "        # Crop and preprocess the face region\n",
    "        face_img = image[y:y+h, x:x+w]\n",
    "        face_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)).resize((224, 224))\n",
    "        inputs = image_processor(images=face_pil, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits[0].softmax(dim=-1)\n",
    "        label = \"With Helmet\" if logits.argmax() == 0 else \"Without Helmet\"\n",
    "        score = logits.max().item()\n",
    "        predictions.append({\"bbox\": (x, y, w, h), \"label\": label, \"score\": score})\n",
    "    return predictions\n",
    "\n",
    "def draw_results(image, results):\n",
    "    \"\"\"Draw bounding boxes and labels on the image.\"\"\"\n",
    "    for result in results:\n",
    "        x, y, w, h = result['bbox']\n",
    "        label = f\"{result['label']} ({result['score']:.2f})\"\n",
    "        color = (0, 255, 0) if \"With Helmet\" in result['label'] else (0, 0, 255)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n",
    "\n",
    "        # Draw label\n",
    "        cv2.putText(image, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return image\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    input_image_path = \"./Media/riders_1.jpg\"\n",
    "    output_image_path = \"./output/detected_faces_helmets.jpg\"\n",
    "\n",
    "    # Read the input image\n",
    "    img = cv2.imread(input_image_path)\n",
    "\n",
    "    # Detect faces\n",
    "    face_boxes = detect_faces_with_mtcnn(img)\n",
    "\n",
    "    # Classify each detected face\n",
    "    results = classify_helmet(img, face_boxes)\n",
    "\n",
    "    # Draw results\n",
    "    output_image = draw_results(img, results)\n",
    "\n",
    "    # Save and display the output\n",
    "    cv2.imwrite(output_image_path, output_image)\n",
    "    cv2.imshow(\"Helmet Detection\", output_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575065fe-dbd4-440a-8efa-c1e9fd7c5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D5\n",
    "import cv2\n",
    "import torch\n",
    "import math\n",
    "from mtcnn import MTCNN  # Install with: pip install mtcnn\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import cvzone  # For stylish bounding boxes\n",
    "\n",
    "# Load the Vision Transformer model\n",
    "model_path = \"./models/helmet_vit\"\n",
    "vit_model = ViTForImageClassification.from_pretrained(model_path)\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Define class labels for Vision Transformer\n",
    "class_labels = ['With Helmet', 'Without Helmet']\n",
    "\n",
    "# Initialize MTCNN for face detection\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Load the image\n",
    "image_path = \"Media/riders_1.jpg\"\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Detect faces using MTCNN\n",
    "detections = mtcnn.detect_faces(img)\n",
    "\n",
    "# Function to preprocess and classify a cropped image\n",
    "def classify_helmet(cropped_img):\n",
    "    \"\"\"Classify whether a cropped image contains a helmet.\"\"\"\n",
    "    # Convert the image to PIL format and resize\n",
    "    face_pil = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)).resize((224, 224))\n",
    "    inputs = image_processor(images=face_pil, return_tensors=\"pt\")\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = vit_model(**inputs)\n",
    "    logits = outputs.logits[0].softmax(dim=-1)\n",
    "    \n",
    "    # Get the predicted label and confidence score\n",
    "    cls_idx = logits.argmax().item()\n",
    "    confidence = logits[cls_idx].item()\n",
    "    return class_labels[cls_idx], confidence\n",
    "\n",
    "# Process each detected face\n",
    "for detection in detections:\n",
    "    box = detection['box']  # Bounding box\n",
    "    x, y, w, h = box\n",
    "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "    \n",
    "    # Crop the face region\n",
    "    face_crop = img[y:y + h, x:x + w]\n",
    "    \n",
    "    # Classify the cropped face\n",
    "    label, confidence = classify_helmet(face_crop)\n",
    "    \n",
    "    # Draw bounding box and label on the image\n",
    "    color = (0, 255, 0) if label == \"With Helmet\" else (0, 0, 255)\n",
    "    cvzone.cornerRect(img, (x, y, w, h), l=10, rt=2, colorR=color)\n",
    "    label_text = f\"{label} {confidence:.2f}\"\n",
    "    cv2.putText(img, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Display the image with detections\n",
    "cv2.imshow(\"Helmet Detection\", img)\n",
    "\n",
    "# Close window when 'q' button is pressed\n",
    "while True:\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4b6ea-cda4-4564-a212-86fed1292c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D6\n",
    "\n",
    "import cv2\n",
    "from mtcnn import MTCNN  # Install with: pip install mtcnn\n",
    "import cvzone  # For stylish bounding boxes\n",
    "\n",
    "# Initialize MTCNN for face detection\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Load the image\n",
    "image_path = \"Media/riders_2.jpg\"  # Replace with the path to your image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Detect faces using MTCNN\n",
    "detections = mtcnn.detect_faces(img)\n",
    "\n",
    "# Process each detected face\n",
    "if len(detections) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for i, detection in enumerate(detections):\n",
    "        # Extract bounding box coordinates\n",
    "        box = detection['box']  # [x, y, width, height]\n",
    "        x, y, w, h = box\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)  # Convert to integers\n",
    "\n",
    "        # Draw bounding box around the face\n",
    "        cvzone.cornerRect(img, (x, y, w, h), l=10, rt=2, colorR=(0, 255, 0))\n",
    "\n",
    "        # Add label with face index\n",
    "        label_text = f\"Face {i + 1}\"\n",
    "        cv2.putText(img, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with face detections\n",
    "cv2.imshow(\"Face Detection\", img)\n",
    "\n",
    "# Wait for a key press to close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d6db2-361a-4cb9-87fa-6083b4349222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D7\n",
    "import cv2\n",
    "from mtcnn import MTCNN  # Install with: pip install mtcnn\n",
    "import cvzone  # For stylish bounding boxes\n",
    "\n",
    "# Initialize MTCNN for face detection\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Load the image\n",
    "image_path = \"Media/riders_5.jpg\"  # Replace with the path to your image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Detect faces using MTCNN\n",
    "detections = mtcnn.detect_faces(img)\n",
    "\n",
    "# Process each detected face\n",
    "if len(detections) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for i, detection in enumerate(detections):\n",
    "        # Extract bounding box coordinates\n",
    "        box = detection['box']  # [x, y, width, height]\n",
    "        x, y, w, h = box\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)  # Convert to integers\n",
    "\n",
    "        # Apply slight padding around the face for better visibility\n",
    "        padding = 10\n",
    "        x = max(x - padding, 0)\n",
    "        y = max(y - padding, 0)\n",
    "        w += padding * 2\n",
    "        h += padding * 2\n",
    "\n",
    "        # Draw bounding box around the face\n",
    "        cvzone.cornerRect(img, (x, y, w, h), l=10, rt=2, colorR=(0, 255, 0))\n",
    "\n",
    "        # Add confidence score and face index as a label\n",
    "        confidence = detection['confidence']  # Confidence score\n",
    "        label_text = f\"Face {i + 1} ({confidence:.2f})\"\n",
    "        cv2.putText(img, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with face detections\n",
    "cv2.imshow(\"Precise Face Detection\", img)\n",
    "\n",
    "# Wait for a key press to close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499eb365-90df-4468-8282-09a6e3509958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D8\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "import cvzone  # For stylish bounding boxes\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN  # Ensure you have MTCNN installed\n",
    "\n",
    "# Initialize the MTCNN for face detection\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Initialize the Vision Transformer (ViT) model and image processor\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"  # You can replace this with a custom-trained model for faces/helmets\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the image\n",
    "image_path = \"Media/riders_3.jpg\"  # Replace with the path to your image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to RGB (ViT expects RGB)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Detect faces using MTCNN\n",
    "mtcnn_detections = mtcnn.detect_faces(img)\n",
    "\n",
    "# Initialize a blank list to collect the face/helmet boxes\n",
    "all_detections = []\n",
    "\n",
    "# Process each detected face from MTCNN\n",
    "if len(mtcnn_detections) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for i, detection in enumerate(mtcnn_detections):\n",
    "        # Extract bounding box coordinates for face\n",
    "        box = detection['box']  # [x, y, width, height]\n",
    "        x, y, w, h = box\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)  # Convert to integers\n",
    "\n",
    "        # Apply slight padding around the face for better visibility\n",
    "        padding = 10\n",
    "        x = max(x - padding, 0)\n",
    "        y = max(y - padding, 0)\n",
    "        w += padding * 2\n",
    "        h += padding * 2\n",
    "\n",
    "        # Add face detection to the list\n",
    "        all_detections.append((x, y, w, h))\n",
    "\n",
    "# Now, process detected faces/helmets with Vision Transformer\n",
    "for i, (x, y, w, h) in enumerate(all_detections):\n",
    "    # Crop the detected region (face or helmet)\n",
    "    cropped_img = img_rgb[y:y+h, x:x+w]\n",
    "\n",
    "    # Preprocess the cropped image for ViT model\n",
    "    inputs = processor(images=cropped_img, return_tensors=\"pt\")\n",
    "\n",
    "    # Make prediction using ViT model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class = logits.argmax(-1).item()\n",
    "\n",
    "    # Assuming '0' = 'Face' and '1' = 'Helmet' in your model classes\n",
    "    if predicted_class == 0:\n",
    "        label = \"Face\"\n",
    "    elif predicted_class == 1:\n",
    "        label = \"Helmet\"\n",
    "    else:\n",
    "        label = \"Unknown\"\n",
    "\n",
    "    # Draw bounding box and label for each detection\n",
    "    cvzone.cornerRect(img, (x, y, w, h), l=10, rt=2, colorR=(0, 255, 0))\n",
    "    label_text = f\"{label} {i + 1}\"\n",
    "    cv2.putText(img, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with face/helmet detections\n",
    "cv2.imshow(\"Face and Helmet Detection using ViT\", img)\n",
    "\n",
    "# Wait for a key press to close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98dcb5-4768-4adf-88c0-2ed7c5299b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D9\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "import cvzone  # For stylish bounding boxes\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN  # Ensure you have MTCNN installed\n",
    "\n",
    "# Initialize the MTCNN for face detection\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Initialize the Vision Transformer (ViT) model and image processor\n",
    "# Use your custom-trained model for helmet detection\n",
    "model_name = \"./models/helmet_vit\"  # Replace with a custom-trained model if you have one\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the image\n",
    "image_path = \"Media/riders_3.jpg\"  # Replace with the path to your image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to RGB (ViT expects RGB)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Detect faces using MTCNN (detect regions that may contain a person)\n",
    "mtcnn_detections = mtcnn.detect_faces(img)\n",
    "\n",
    "# Initialize a blank list to collect the face/helmet boxes\n",
    "all_detections = []\n",
    "\n",
    "# Process each detected face from MTCNN\n",
    "if len(mtcnn_detections) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for i, detection in enumerate(mtcnn_detections):\n",
    "        # Extract bounding box coordinates for the detected face\n",
    "        box = detection['box']  # [x, y, width, height]\n",
    "        x, y, w, h = box\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)  # Convert to integers\n",
    "\n",
    "        # Apply slight padding around the face for better visibility\n",
    "        padding = 10\n",
    "        x = max(x - padding, 0)\n",
    "        y = max(y - padding, 0)\n",
    "        w += padding * 2\n",
    "        h += padding * 2\n",
    "\n",
    "        # Add face detection to the list\n",
    "        all_detections.append((x, y, w, h))\n",
    "\n",
    "# Now, process detected faces/helmet regions with Vision Transformer\n",
    "for i, (x, y, w, h) in enumerate(all_detections):\n",
    "    # Crop the detected region (face or helmet area)\n",
    "    cropped_img = img_rgb[y:y+h, x:x+w]\n",
    "\n",
    "    # Preprocess the cropped image for ViT model\n",
    "    inputs = processor(images=cropped_img, return_tensors=\"pt\")\n",
    "\n",
    "    # Make prediction using ViT model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class = logits.argmax(-1).item()\n",
    "\n",
    "    # Assuming '0' = 'No Helmet' and '1' = 'Helmet' in your model's output\n",
    "    if predicted_class == 1:\n",
    "        label = \"Helmet\"\n",
    "    else:\n",
    "        label = \"No Helmet\"\n",
    "\n",
    "    # Draw bounding box and label for each detection\n",
    "    cvzone.cornerRect(img, (x, y, w, h), l=10, rt=2, colorR=(0, 255, 0))\n",
    "    label_text = f\"{label} {i + 1}\"\n",
    "    cv2.putText(img, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with helmet detection\n",
    "cv2.imshow(\"Helmet Detection using ViT\", img)\n",
    "\n",
    "# Wait for a key press to close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d509f5-6855-4a83-aaf5-8fb35480ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D10\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "import cvzone  # For stylish bounding boxes\n",
    "from mtcnn import MTCNN  # Ensure you have MTCNN installed\n",
    "\n",
    "# Initialize the MTCNN for face detection\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Initialize the Vision Transformer (ViT) model and image processor\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"  # Replace with a custom-trained model for helmet detection\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the image\n",
    "image_path = \"Media/riders_3.jpg\"  # Replace with the path to your image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to RGB (ViT expects RGB)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Detect faces using MTCNN (detect regions that may contain a person)\n",
    "mtcnn_detections = mtcnn.detect_faces(img)\n",
    "\n",
    "# Initialize a blank list to collect the face/helmet boxes\n",
    "all_detections = []\n",
    "\n",
    "# Process each detected face from MTCNN\n",
    "if len(mtcnn_detections) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for i, detection in enumerate(mtcnn_detections):\n",
    "        # Extract bounding box coordinates for the detected face\n",
    "        box = detection['box']  # [x, y, width, height]\n",
    "        x, y, w, h = box\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)  # Convert to integers\n",
    "\n",
    "        # Apply slight padding around the face for better visibility\n",
    "        padding = 10\n",
    "        x = max(x - padding, 0)\n",
    "        y = max(y - padding, 0)\n",
    "        w += padding * 2\n",
    "        h += padding * 2\n",
    "\n",
    "        # Add face detection to the list\n",
    "        all_detections.append((x, y, w, h))\n",
    "\n",
    "# Now, process detected faces/helmet regions with Vision Transformer\n",
    "for i, (x, y, w, h) in enumerate(all_detections):\n",
    "    # Crop the detected region (face or helmet area)\n",
    "    cropped_img = img_rgb[y:y+h, x:x+w]\n",
    "\n",
    "    # Preprocess the cropped image for ViT model\n",
    "    inputs = processor(images=cropped_img, return_tensors=\"pt\")\n",
    "\n",
    "    # Make prediction using ViT model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class = logits.argmax(-1).item()\n",
    "\n",
    "    # Assuming '0' = 'No Helmet' and '1' = 'Helmet' in your model's output\n",
    "    if predicted_class == 1:\n",
    "        label = \"Helmet\"\n",
    "    else:\n",
    "        label = \"No Helmet\"\n",
    "\n",
    "    # Draw bounding box and label for each detection\n",
    "    cvzone.cornerRect(img, (x, y, w, h), l=10, rt=2, colorR=(0, 255, 0))\n",
    "    label_text = f\"Person {i + 1}: {label}\"\n",
    "    cv2.putText(img, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image with helmet detection\n",
    "cv2.imshow(\"Helmet Detection for Each Individual\", img)\n",
    "\n",
    "# Wait for a key press to close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311be515-2ae8-47bf-b530-9240fd82ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D11\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "import cvzone  # For stylish bounding boxes\n",
    "from mtcnn import MTCNN  # Ensure you have MTCNN installed\n",
    "\n",
    "# Initialize the MTCNN for face detection\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "# Initialize the Vision Transformer (ViT) model and image processor\n",
    "model_name = \"./models/helmet_vit\"  # Replace with a custom-trained model for helmet detection\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load the image\n",
    "image_path = \"Media/riders_6.jpg\"  # Replace with the path to your image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to RGB (ViT expects RGB)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Detect faces using MTCNN (detect regions that may contain a person)\n",
    "mtcnn_detections = mtcnn.detect_faces(img)\n",
    "\n",
    "# Initialize a blank list to collect the face/helmet boxes\n",
    "all_detections = []\n",
    "\n",
    "# Process each detected face from MTCNN\n",
    "if len(mtcnn_detections) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for detection in mtcnn_detections:\n",
    "        # Extract bounding box coordinates for the detected face\n",
    "        box = detection['box']  # [x, y, width, height]\n",
    "        x, y, w, h = box\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)  # Convert to integers\n",
    "\n",
    "        # Apply slight padding around the face for better visibility\n",
    "        padding = 10\n",
    "        x = max(x - padding, 0)\n",
    "        y = max(y - padding, 0)\n",
    "        w += padding * 2\n",
    "        h += padding * 2\n",
    "\n",
    "        # Add face detection to the list\n",
    "        all_detections.append((x, y, w, h))\n",
    "\n",
    "# Now, process detected faces/helmet regions with Vision Transformer\n",
    "for x, y, w, h in all_detections:\n",
    "    # Crop the detected region (face or helmet area)\n",
    "    cropped_img = img_rgb[y:y+h, x:x+w]\n",
    "\n",
    "    # Preprocess the cropped image for ViT model\n",
    "    inputs = processor(images=cropped_img, return_tensors=\"pt\")\n",
    "\n",
    "    # Make prediction using ViT model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class = logits.argmax(-1).item()\n",
    "\n",
    "    # Assuming '0' = 'No Helmet' and '1' = 'Helmet' in your model's output\n",
    "    if predicted_class == 1:\n",
    "        label = \"Helmet\"\n",
    "        color = (0, 255, 0)  # Green for Helmet\n",
    "    else:\n",
    "        label = \"No Helmet\"\n",
    "        color = (0, 0, 255)  # Red for No Helmet\n",
    "\n",
    "    # Draw bounding box and label for each detection\n",
    "    cvzone.cornerRect(img, (x, y, w, h), l=10, rt=2, colorR=color)\n",
    "    cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Display the image with helmet detection\n",
    "cv2.imshow(\"Helmet Detection\", img)\n",
    "\n",
    "# Wait for a key press to close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22f284-61ab-4ffa-a6a1-8a99b5ab6583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
